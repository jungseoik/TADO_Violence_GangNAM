{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pia/anaconda3/envs/vl3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pia/anaconda3/envs/vl3/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL3-2B:\n",
      "- configuration_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL3-2B:\n",
      "- configuration_internvl_chat.py\n",
      "- configuration_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL3-2B:\n",
      "- conversation.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL3-2B:\n",
      "- modeling_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL3-2B:\n",
      "- modeling_internvl_chat.py\n",
      "- conversation.py\n",
      "- modeling_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/home/pia/anaconda3/envs/vl3/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Downloading test image from https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/det.jpg...\n",
      "\n",
      "--- Starting GPU Memory Measurement (Transformers) ---\n",
      "Base model memory usage: 4441.27 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  1 -> Peak GPU Memory: 6150.02 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  2 -> Peak GPU Memory: 7832.28 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  3 -> Peak GPU Memory: 9512.52 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  4 -> Peak GPU Memory: 11190.04 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  5 -> Peak GPU Memory: 12857.67 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  6 -> Peak GPU Memory: 14534.15 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  7 -> Peak GPU Memory: 16210.63 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  8 -> Peak GPU Memory: 17887.47 MB\n",
      "Batch Size:  9 -> Peak GPU Memory: 19563.96 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 10 -> Peak GPU Memory: 21240.70 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "import math\n",
    "import os\n",
    "\n",
    "# --- 제공해주신 전처리 함수들 (수정 없이 사용) ---\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image_pixels(image_file, input_size=448, max_num=6):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "# --- 실험 설정 ---\n",
    "if __name__ == '__main__':\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available. This script requires a GPU.\")\n",
    "        exit()\n",
    "\n",
    "    # 모델 로드\n",
    "    model_path = \"OpenGVLab/InternVL3-2B\"\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    ).eval().cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    # 테스트용 이미지 다운로드\n",
    "    image_url = \"https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/det.jpg\"\n",
    "    image_path = \"test_image.jpg\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Downloading test image from {image_url}...\")\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(image_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "    # 이미지 전처리 (한 번만 수행)\n",
    "    single_image_pixels = load_image_pixels(image_path, max_num=6).to(torch.bfloat16).cuda()\n",
    "\n",
    "    # 실험 파라미터\n",
    "    question = '<image>\\nDescribe the image in detail.'\n",
    "    generation_config = dict(max_new_tokens=50, do_sample=False)\n",
    "    max_batch_size = 10\n",
    "\n",
    "    print(\"\\n--- Starting GPU Memory Measurement (Transformers) ---\")\n",
    "    \n",
    "    # 모델 로드 후 기본 메모리 사용량 측정\n",
    "    torch.cuda.empty_cache()\n",
    "    base_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "    print(f\"Base model memory usage: {base_memory:.2f} MB\")\n",
    "\n",
    "    for batch_size in range(1, max_batch_size + 1):\n",
    "        # 배치 데이터 준비\n",
    "        pixel_values = torch.cat([single_image_pixels] * batch_size, dim=0)\n",
    "        num_patches_list = [single_image_pixels.size(0)] * batch_size\n",
    "        questions = [question] * batch_size\n",
    "        \n",
    "        # 메모리 측정 시작\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        # 추론 실행\n",
    "        with torch.no_grad():\n",
    "            responses = model.batch_chat(\n",
    "                tokenizer,\n",
    "                pixel_values=pixel_values,\n",
    "                num_patches_list=num_patches_list,\n",
    "                questions=questions,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "        \n",
    "        # 피크 메모리 측정 (MB 단위)\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        \n",
    "        print(f\"Batch Size: {batch_size:2d} -> Peak GPU Memory: {peak_memory:.2f} MB\")\n",
    "        \n",
    "        # 다음 배치를 위해 변수 삭제 및 캐시 정리\n",
    "        del pixel_values, num_patches_list, questions, responses\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 설명                    | V2 결과 (기존)      | V3 결과 (수정)     \n",
      "-----------------------------------------------------------------\n",
      "정상 (소문자)                  | normal          | normal         \n",
      "정상 (소문자)                  | violence        | violence       \n",
      "대소문자: Title Case          | no_json_found   | violence       \n",
      "대소문자: ALL CAPS            | no_json_found   | normal         \n",
      "대소문자: MiXeD CaSe          | no_json_found   | violence       \n",
      "대소문자: Regex Fallback      | no_json_found   | normal         \n",
      "마크다운 포함                   | normal          | normal         \n",
      "앞뒤 텍스트 포함                 | violence        | violence       \n",
      "JSON 형식 깨짐                | normal          | normal         \n",
      "category 키 없음             | no_json_found   | no_json_found  \n",
      "완전한 쓰레기값                  | no_json_found   | no_json_found  \n",
      "빈 문자열                     | no_json_found   | no_json_found  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# 이전 버전 (v2) - 비교를 위해 포함\n",
    "def parse_prediction_v2(pred_str: str) -> str:\n",
    "    if not isinstance(pred_str, str): return 'parsing_failed'\n",
    "    try:\n",
    "        clean_str = pred_str\n",
    "        if '```json' in clean_str: clean_str = clean_str.split('```json')[1].split('```')[0]\n",
    "        elif '```' in clean_str: clean_str = clean_str.split('```')[1].split('```')[0]\n",
    "        clean_str = clean_str.strip()\n",
    "        start_brace, end_brace = clean_str.find('{'), clean_str.rfind('}')\n",
    "        if start_brace != -1 and end_brace != -1 and start_brace < end_brace:\n",
    "            json_part = clean_str[start_brace : end_brace + 1]\n",
    "            try:\n",
    "                data = json.loads(json_part)\n",
    "                category = data.get('category')\n",
    "                if category in ['violence', 'normal']: return category\n",
    "            except json.JSONDecodeError: pass\n",
    "        cat_match = re.search(r'[\"\\']category[\"\\']\\s*:\\s*[\"\\'](violence|normal)[\"\\']', clean_str)\n",
    "        if cat_match: return cat_match.group(1)\n",
    "        return 'no_json_found'\n",
    "    except Exception: return 'parsing_failed'\n",
    "\n",
    "# 대소문자 무시 버전 (v3)\n",
    "def parse_prediction_v3(pred_str: str) -> str:\n",
    "    if not isinstance(pred_str, str): return 'parsing_failed'\n",
    "    try:\n",
    "        clean_str = pred_str\n",
    "        if '```json' in clean_str: clean_str = clean_str.split('```json')[1].split('```')[0]\n",
    "        elif '```' in clean_str: clean_str = clean_str.split('```')[1].split('```')[0]\n",
    "        clean_str = clean_str.strip()\n",
    "        start_brace, end_brace = clean_str.find('{'), clean_str.rfind('}')\n",
    "        if start_brace != -1 and end_brace != -1 and start_brace < end_brace:\n",
    "            json_part = clean_str[start_brace : end_brace + 1]\n",
    "            try:\n",
    "                data = json.loads(json_part)\n",
    "                category = data.get('category')\n",
    "                if isinstance(category, str) and category.lower() in ['violence', 'normal']:\n",
    "                    return category.lower()\n",
    "            except json.JSONDecodeError: pass\n",
    "        cat_match = re.search(r'[\"\\']category[\"\\']\\s*:\\s*[\"\\'](violence|normal)[\"\\']', clean_str, re.IGNORECASE)\n",
    "        if cat_match: return cat_match.group(1).lower()\n",
    "        return 'no_json_found'\n",
    "    except Exception: return 'parsing_failed'\n",
    "\n",
    "\n",
    "# ----------------- 테스트 케이스 정의 (대소문자 케이스 추가) -----------------\n",
    "test_cases = [\n",
    "    (\"정상 (소문자)\", '{\"category\": \"normal\", \"description\": \"...\"}'),\n",
    "    (\"정상 (소문자)\", '{\"category\": \"violence\", \"description\": \"...\"}'),\n",
    "    (\"대소문자: Title Case\", '{\"category\": \"Violence\", \"description\": \"...\"}'),\n",
    "    (\"대소문자: ALL CAPS\", '{\"category\": \"NORMAL\", \"description\": \"...\"}'),\n",
    "    (\"대소문자: MiXeD CaSe\", '{\"category\": \"vIoLeNcE\", \"description\": \"...\"}'),\n",
    "    (\"대소문자: Regex Fallback\", \"'category': 'nOrMaL', 'description': '...'\"),\n",
    "    (\"마크다운 포함\", '```json\\n{\"category\": \"normal\", \"description\": \"...\"}\\n```'),\n",
    "    (\"앞뒤 텍스트 포함\", 'Answer: {\"category\": \"violence\", \"description\": \"...\"}'),\n",
    "    (\"JSON 형식 깨짐\", '{\"category\": \"normal\" \"description\": \"...\"}'),\n",
    "    (\"category 키 없음\", '{\"action\": \"running\", \"description\": \"...\"}'),\n",
    "    (\"완전한 쓰레기값\", \"A cat playing with a ball.\"),\n",
    "    (\"빈 문자열\", \"\"),\n",
    "]\n",
    "\n",
    "# ----------------- 테스트 실행 및 결과 비교 -----------------\n",
    "print(f\"{'테스트 설명':<25} | {'V2 결과 (기존)':<15} | {'V3 결과 (수정)':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for desc, case in test_cases:\n",
    "    res2 = parse_prediction_v2(case)\n",
    "    res3 = parse_prediction_v3(case)\n",
    "    print(f\"{desc:<25} | {res2:<15} | {res3:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from decord import VideoReader, cpu\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "# 공통 설정\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size: int = 448):\n",
    "    return T.Compose([\n",
    "        T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float(\"inf\")\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        tgt_ar = ratio[0] / ratio[1]\n",
    "        diff = abs(aspect_ratio - tgt_ar)\n",
    "        if diff < best_ratio_diff or (diff == best_ratio_diff and area > 0.5 * image_size * image_size * ratio[0] * ratio[1]):\n",
    "            best_ratio_diff = diff\n",
    "            best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    ow, oh = image.size\n",
    "    aspect_ratio = ow / oh\n",
    "    target_ratios = sorted(\n",
    "        {(i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if min_num <= i * j <= max_num},\n",
    "        key=lambda x: x[0] * x[1],\n",
    "    )\n",
    "    ratio = find_closest_aspect_ratio(aspect_ratio, target_ratios, ow, oh, image_size)\n",
    "    tw, th = image_size * ratio[0], image_size * ratio[1]\n",
    "    blocks = ratio[0] * ratio[1]\n",
    "    resized = image.resize((tw, th))\n",
    "    tiles = [\n",
    "        resized.crop((\n",
    "            (idx % (tw // image_size)) * image_size,\n",
    "            (idx // (tw // image_size)) * image_size,\n",
    "            ((idx % (tw // image_size)) + 1) * image_size,\n",
    "            ((idx // (tw // image_size)) + 1) * image_size,\n",
    "        ))\n",
    "        for idx in range(blocks)\n",
    "    ]\n",
    "    if use_thumbnail and blocks != 1:\n",
    "        tiles.append(image.resize((image_size, image_size)))\n",
    "    return tiles\n",
    "\n",
    "# V1 프레임 샘플링 방식 (첫 번째 코드)\n",
    "def get_indices_by_frame_range_v1(start_idx: int, end_idx: int, num_segments: int) -> np.ndarray:\n",
    "    start = int(start_idx)\n",
    "    end = int(end_idx)\n",
    "    if end < start:\n",
    "        end = start\n",
    "    length = end - start + 1\n",
    "    num = max(1, min(num_segments, length))\n",
    "    step = length / float(num)\n",
    "    idxs = [start + int(step * i + step / 2) for i in range(num)]\n",
    "    idxs = [min(max(start, x), end) for x in idxs]\n",
    "    return np.array(idxs, dtype=int)\n",
    "\n",
    "# V2 프레임 샘플링 방식 (두 번째 코드)\n",
    "def get_index_v2(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, start)\n",
    "    end_idx = min(end, max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([\n",
    "        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "        for idx in range(num_segments)\n",
    "    ])\n",
    "    return frame_indices\n",
    "\n",
    "def load_video_v1(video_path: str, start_frame: int, end_frame: int, \n",
    "                  input_size: int = 448, max_num: int = 1, num_segments: int = 12):\n",
    "    \"\"\"V1 방식으로 비디오 로드\"\"\"\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame_idx = len(vr) - 1\n",
    "    s = max(0, min(start_frame, max_frame_idx))\n",
    "    e = max(0, min(end_frame, max_frame_idx))\n",
    "    indices = get_indices_by_frame_range_v1(s, e, num_segments=num_segments)\n",
    "    \n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    for frame_index in indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\n",
    "        tiles = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in tiles]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list, indices\n",
    "\n",
    "def load_video_v2(video_path: str, bound=None, input_size=448, max_num=1, num_segments=32):\n",
    "    \"\"\"V2 방식으로 비디오 로드\"\"\"\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "    \n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    frame_indices = get_index_v2(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "    \n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list, frame_indices\n",
    "\n",
    "class InternVL3Inferencer:\n",
    "    def __init__(self, model_path=\"OpenGVLab/InternVL3-2B\", device=\"cuda:0\"):\n",
    "        print(f\"[INFO] InternVL 모델 로딩 중... device={device}\")\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_flash_attn=False,\n",
    "            trust_remote_code=True\n",
    "        ).eval().to(device)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n",
    "        self.device = device\n",
    "        self.generation_config = dict(max_new_tokens=1024, do_sample=False)\n",
    "        print(f\"[INFO] InternVL 모델 로딩 완료.\")\n",
    "\n",
    "    def infer_v1(self, video_path: str, prompt: str, start_frame: int, end_frame: int, num_segments: int = 12):\n",
    "        \"\"\"V1 방식 추론\"\"\"\n",
    "        pixel_values, num_patches_list, indices = load_video_v1(\n",
    "            video_path, start_frame, end_frame, input_size=448, max_num=1, num_segments=num_segments\n",
    "        )\n",
    "        pixel_values = pixel_values.to(torch.bfloat16).to(self.device)\n",
    "        video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\n",
    "        question = video_prefix + prompt\n",
    "        response = self.model.chat(self.tokenizer, pixel_values, question, self.generation_config)\n",
    "        return response, indices\n",
    "\n",
    "    def infer_v2(self, video_path: str, template: str, num_segments: int = 12, bound=None):\n",
    "        \"\"\"V2 방식 추론\"\"\"\n",
    "        pixel_values, num_patches_list, indices = load_video_v2(\n",
    "            video_path, bound=bound, num_segments=num_segments, max_num=1\n",
    "        )\n",
    "        pixel_values = pixel_values.to(torch.bfloat16).to(self.device)\n",
    "        video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\n",
    "        question = video_prefix + template\n",
    "        response = self.model.chat(self.tokenizer, pixel_values, question, self.generation_config)\n",
    "        return response, indices\n",
    "\n",
    "def compare_inference_results(video_path: str, model_path: str = \"OpenGVLab/InternVL3-2B\"):\n",
    "    \"\"\"두 버전의 추론 결과를 비교\"\"\"\n",
    "    \n",
    "    # 테스트 설정\n",
    "    START_FRAME = 1224\n",
    "    END_FRAME = 1235\n",
    "    NUM_SEGMENTS = 12\n",
    "    PROMPT    = \"\"\"\n",
    "    Watch this short video clip and respond with exactly one JSON object.\\n\\n[Rules]\\n- The category must be either 'violence' or 'normal'.  \\n- Classify as violence if any of the following actions are present:  \\n  * Punching  \\n  * Kicking  \\n  * Weapon Threat\\n  * Weapon Attack\\n  * Falling/Takedown  \\n  * Pushing/Shoving  \\n  * Brawling/Group Fight  \\n- If none of the above are observed, classify as normal.  \\n- The following cases must always be classified as normal:  \\n  * Affection (hugging, holding hands, light touches)  \\n  * Helping (supporting, assisting)  \\n  * Accidental (unintentional bumping)  \\n  * Playful (non-aggressive playful contact)  \\n\\n[Output Format]\\n- Output exactly one JSON object.  \\n- The object must contain only two keys: \\\"category\\\" and \\\"description\\\".  \\n- The description should briefly and objectively describe the scene.  \\n\\nExample (violence):  \\n{\\\"category\\\":\\\"violence\\\",\\\"description\\\":\\\"A man in a black jacket punches another man, who stumbles backward.\\\"}\\n\\nExample (normal):  \\n{\\\"category\\\":\\\"normal\\\",\\\"description\\\":\\\"Two people are hugging inside an elevator\n",
    "    \"\"\"  \n",
    "    PROMPT = \"\"\"\n",
    "    Watch this short video clip (1–2 seconds) and respond with exactly one JSON object.\\n\\n[Rules]\\n- The category must be either 'violence' or 'normal'.  \\n- Classify as violence if any of the following actions are present:  \\n  * Punching  \\n  * Kicking  \\n  * Weapon Threat  \\n  * Falling/Takedown  \\n  * Pushing/Shoving  \\n  * Brawling/Group Fight  \\n- If none of the above are observed, classify as normal.  \\n- The following cases must always be classified as normal:  \\n  * Affection (hugging, holding hands, light touches)  \\n  * Helping (supporting, assisting)  \\n  * Accidental (unintentional bumping)  \\n  * Playful (non-aggressive playful contact)  \\n  * Sports (contact within sports rules)  \\n\\n[Output Format]\\n- Output exactly one JSON object.  \\n- The object must contain only two keys: \\\"category\\\" and \\\"description\\\".  \\n- The description should briefly and objectively describe the scene.  \\n\\nExample (violence):  \\n{\\\"category\\\":\\\"violence\\\",\\\"description\\\":\\\"A man in a black jacket punches another man, who stumbles backward.\\\"}\\n\\nExample (normal):  \\n{\\\"category\\\":\\\"normal\\\",\\\"description\\\":\\\"Two people are hugging inside an elevator.\n",
    "    \"\"\"\n",
    "    PROMPT    = \"\"\"\n",
    "    Watch this short video clip and respond with exactly one JSON object.\\n\\n[Rules]\\n- The category must be either 'violence' or 'normal'.  \\n- Classify as violence if any of the following actions are present:  \\n  * Punching  \\n  * Kicking  \\n  * Weapon Threat\\n  * Weapon Attack\\n  * Falling/Takedown  \\n  * Pushing/Shoving  \\n  * Brawling/Group Fight  \\n- If none of the above are observed, classify as normal.  \\n- The following cases must always be classified as normal:  \\n  * Affection (hugging, holding hands, light touches)  \\n  * Helping (supporting, assisting)  \\n  * Accidental (unintentional bumping)  \\n  * Playful (non-aggressive playful contact)  \\n\\n[Output Format]\\n- Output exactly one JSON object.  \\n- The object must contain only two keys: \\\"category\\\" and \\\"description\\\".  \\n- The description should briefly and objectively describe the scene.  \\n\\nExample (violence):  \\n{\\\"category\\\":\\\"violence\\\",\\\"description\\\":\\\"A man in a black jacket punches another man, who stumbles backward.\\\"}\\n\\nExample (normal):  \\n{\\\"category\\\":\\\"normal\\\",\\\"description\\\":\\\"Two people are hugging inside an elevator\"}\n",
    "    \"\"\"  \n",
    "#     PROMPT = \"\"\"Watch this short video clip and respond with exactly one JSON object.\n",
    "\n",
    "# [Rules]\n",
    "# - The category must be either 'violence' or 'normal'.\n",
    "# - Classify as violence if any of the following actions are present:\n",
    "#   * Punching\n",
    "#   * Kicking\n",
    "#   * Weapon Threat\n",
    "#   * Weapon Attack\n",
    "#   * Falling/Takedown\n",
    "#   * Pushing/Shoving\n",
    "#   * Brawling/Group Fight\n",
    "# - If none of the above are observed, classify as normal.\n",
    "# - The following cases must always be classified as normal:\n",
    "#   * Affection (hugging, holding hands, light touches)\n",
    "#   * Helping (supporting, assisting)\n",
    "#   * Accidental (unintentional bumping)\n",
    "#   * Playful (non-aggressive playful contact)\n",
    "\n",
    "# [Output Format]\n",
    "# - Output exactly one JSON object.\n",
    "# - The object must contain only two keys: \"category\" and \"description\".\n",
    "# - The description should briefly and objectively describe the scene.\n",
    "\n",
    "# Example (violence):\n",
    "# {\"category\":\"violence\",\"description\":\"A man in a black jacket punches another man, who stumbles backward.\"}\n",
    "\n",
    "# Example (normal):\n",
    "# {\"category\":\"normal\",\"description\":\"Two people are hugging inside an elevator\"}\n",
    "# \"\"\"\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"비디오 추론 결과 비교 테스트\")\n",
    "    print(f\"비디오: {video_path}\")\n",
    "    print(f\"프레임 구간: {START_FRAME} - {END_FRAME}\")\n",
    "    print(f\"NUM_SEGMENTS: {NUM_SEGMENTS}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 모델 로드\n",
    "    inferencer = InternVL3Inferencer(model_path)\n",
    "    \n",
    "    # V1 방식 테스트\n",
    "    print(\"\\n[V1 방식 테스트]\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result_v1, indices_v1 = inferencer.infer_v1(\n",
    "            video_path, PROMPT, START_FRAME, END_FRAME, NUM_SEGMENTS\n",
    "        )\n",
    "        v1_time = time.time() - start_time\n",
    "        print(f\"선택된 프레임 인덱스: {indices_v1.tolist()}\")\n",
    "        print(f\"추론 시간: {v1_time:.2f}초\")\n",
    "        print(f\"결과: {result_v1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"V1 방식 에러: {e}\")\n",
    "        result_v1, indices_v1 = None, None\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    \n",
    "    # V2 방식 테스트\n",
    "    print(\"\\n[V2 방식 테스트]\")\n",
    "    bound = [START_FRAME, END_FRAME]\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result_v2, indices_v2 = inferencer.infer_v2(\n",
    "            video_path, PROMPT, NUM_SEGMENTS, bound\n",
    "        )\n",
    "        v2_time = time.time() - start_time\n",
    "        print(f\"선택된 프레임 인덱스: {indices_v2.tolist()}\")\n",
    "        print(f\"추론 시간: {v2_time:.2f}초\")\n",
    "        print(f\"결과: {result_v2}\")\n",
    "    except Exception as e:\n",
    "        print(f\"V2 방식 에러: {e}\")\n",
    "        result_v2, indices_v2 = None, None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # 비교 분석\n",
    "    print(\"\\n[비교 분석]\")\n",
    "    \n",
    "    if indices_v1 is not None and indices_v2 is not None:\n",
    "        print(f\"프레임 인덱스 동일성: {np.array_equal(indices_v1, indices_v2)}\")\n",
    "        if not np.array_equal(indices_v1, indices_v2):\n",
    "            print(f\"V1 인덱스: {indices_v1.tolist()}\")\n",
    "            print(f\"V2 인덱스: {indices_v2.tolist()}\")\n",
    "            print(f\"차이점: {set(indices_v1) - set(indices_v2)} (V1에만 있음)\")\n",
    "            print(f\"차이점: {set(indices_v2) - set(indices_v1)} (V2에만 있음)\")\n",
    "    \n",
    "    if result_v1 is not None and result_v2 is not None:\n",
    "        print(f\"\\n결과 동일성: {result_v1 == result_v2}\")\n",
    "        if result_v1 != result_v2:\n",
    "            print(f\"\\nV1 결과:\\n{result_v1}\")\n",
    "            print(f\"\\nV2 결과:\\n{result_v2}\")\n",
    "            \n",
    "            # JSON 파싱해서 category 비교\n",
    "            try:\n",
    "                import re\n",
    "                def extract_category(text):\n",
    "                    match = re.search(r'\"category\"\\s*:\\s*\"([^\"]*)\"', text)\n",
    "                    return match.group(1) if match else \"파싱 실패\"\n",
    "                \n",
    "                cat_v1 = extract_category(result_v1)\n",
    "                cat_v2 = extract_category(result_v2)\n",
    "                print(f\"\\n분류 결과 - V1: {cat_v1}, V2: {cat_v2}\")\n",
    "                print(f\"분류 일치: {cat_v1 == cat_v2}\")\n",
    "            except:\n",
    "                print(\"분류 결과 파싱 실패\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 사용 예시\n",
    "    VIDEO_PATH = \"sample/fight_0162.mp4\"  # 실제 경로로 변경\n",
    "    MODEL_PATH = \"ckpts/PIA_Violence\"  # 또는 사용하는 모델 경로\n",
    "    \n",
    "    if not os.path.exists(VIDEO_PATH):\n",
    "        print(f\"비디오 파일을 찾을 수 없습니다: {VIDEO_PATH}\")\n",
    "        print(\"VIDEO_PATH를 실제 파일 경로로 수정하세요.\")\n",
    "    else:\n",
    "        compare_inference_results(VIDEO_PATH, MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
